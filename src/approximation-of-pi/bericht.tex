\documentclass{scrartcl}
\usepackage{scrhack}
\subject{Bericht}
\titlehead{%
  Humboldt-Universität zu Berlin\\
  Mathematisch-Naturwissenschaftliche Fakultät\\
  Institut für Mathematik
}
\title{Annäherungen von \(\pi\)}
\author{
  Eingereicht von M. van Straten und P. Merz
}
\date{\today} % This will be set to the data of the last modification by the build-system 

\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{svg}

\addbibresource{./bericht.bib}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{approximation sequence}{Annäherungsfolge}
\newtheorem{observation}{Beobachtung}

\parskip = \baselineskip
\parindent = 0pt

\begin{document}
\maketitle
\cleardoublepage{}
\tableofcontents
\cleardoublepage{}

\section{Einleitung}

Die Konstante \(\pi\) hat eine natürliche Definition in der euklidischen
Geometrie als das Verhältnis zwischen dem Umfang und dem Durchmesser eines
Kreises. Sie ist von fundamentaler Bedeutung in vielen Bereichen der Mathematik
und Physik. Beispielsweise tritt \(\pi\) im Gaußschen Integral auf, bei den
komplexen Einheitswurzeln und in der Cauchy-Verteilung in der
Wahrscheinlichkeitstheorie. Die Relevanz von \(\pi\) erstreckt sich dabei über
zahlreiche Disziplinen hinweg, was sie zu einer der wichtigsten Konstanten in
der Wissenschaft macht.

\subsection{Historie}
Historisch gesehen haben sich Mathematiker stets bemüht, \(\pi\) immer genauer
zu approximieren.
In der Antike reichte die Genauigkeit solcher Annäherungen oft nur bis zur
zweiten Nachkommastelle.
Signifikante Fortschritte wurden im 14.
Jahrhundert durch Madhava von Sangamagrama erzielt, der Methoden entwickelte,
um \(\pi\) bis auf elf und später dreizehn Stellen genau zu berechnen.
In der chinesischen Mathematik wurde im 5.
Jahrhundert eine Genauigkeit von etwa sieben Dezimalstellen erreicht.
Ende des 20. Jahrhunderts hatte sich diese Präzision auf 2 Millionen Dezimalstellen, erreicht durch Jean Guilloud im Jahr 1981, erhöht und
ist innerhalb weniger Jahre bis 1988 auf das zehnfache, ca. 200 Millionen Nachkommastellen, angestiegen.
Bis zum heutigen Jahr gab es viele Fortschritte in diesem Fachgebiet, so wurde \(\pi\) 2009 von Fabrice Bellard auf 2,699,999,990,000 Dezimalstellen approximiert und
die neueste Entwicklung ist von März 2024, als Jordan Ranous, Kevin O’Brien und Brian Beeler es sogar schafften \(\pi\) auf 105,000,000,000,000 Dezimalstellen zu approximieren. \cite{Chronology}

\subsection{Motivation}
Diese historischen Entwicklungen zeigen die Bedeutung und den anhaltenden
wissenschaftlichen Fortschritt in der Annäherung von \(\pi\). Die genaue
Berechnung von \(\pi\) bleibt nicht nur eine mathematische Herausforderung,
sondern ist auch für viele praktische Anwendungen relevant, etwa in der
Ingenieurwissenschaft, der Physik und der Informatik.

Ziel dieses Berichts ist es, verschiedene mathematische Annäherungsmethoden zur
Berechnung von \(\pi\) zu untersuchen und miteinander zu vergleichen.
Neben der bereits bekannten Leibniz-Reihe werden zwei weitere Verfahren
implementiert und analysiert.
Diese Verfahren werden hinsichtlich ihrer Laufzeit, ihres Speicherbedarfs,
ihres Konvergenzverhaltens und des Verhältnisses von Rechenaufwand zu
Approximationsgenauigkeit sowohl analytisch als auch experimentell verglichen.


\section{Theorie}

\subsection{Einführung und Einordnung von Fachbegriffen}

\begin{definition}[Annäherungsfolge]
    Sei \((a_n)_{n \in \mathbb{N}}\) eine Folge reeller Zahlen und \(P \in \mathbb{R}\), sodass
    \[\lim_{n \to \infty} a_n = P \] gilt. Die Folge \(a_n\) heißt dann Annäherungsfolge für den Punkt \(P\)

\end{definition}

\begin{definition}[Uniforme Teilmenge]
    Eine uniforme Teilmenge \(U\) einer Menge \(M\) ist eine Teilmenge, in der
    jedes Element mit gleicher Wahrscheinlichkeit ausgewählt wird.
    Dies bedeutet, dass für jedes \(x \in M\) die Wahrscheinlichkeit \(P(x \in U)\)
    konstant ist.
\end{definition}

\begin{definition}[Konvergenzgeschwindigkeit \cite{Konvergenzgeschwindigkeit}]
    Sei \((s_n)_{n \in \mathbb{N}}\) eine Approximationsfolge mit Grenzwert s, wobei O. B. d. A. alle Folgeglieder paarweise verschieden und ungleich dem Grenzwert selbst sind.
    Dann konvergiert \(s_n\) linear, falls
    \[\limsup_{n \to \infty} c_n < 1 \text{ mit } c_n := \frac{|s_{n+1}-s|}{|s_k-s|} \]
    Falls c = 1, so konvergiert die Folge sublinear.
    Gilt zusätzlich 
    \[\lim_{n \to \infty} \frac{|s_{n+2}-s_{n+1}|}{|s_{n+1}-s_n|} = 1\]
    so heißt die Folge logarithmisch konvergent.
    c wird öfter auch als Konvergenzrate bezeichnet, denn je kleiner c, desto schneller konvergiert die Folge gegen ihren Grenzwert, 
    d.h. für eine gewünschte Präzision werden weniger Iterationen benötigt.
    Zudem lässt sich die Konvergenz der Ordnung q wie folgt definieren:
    Eine Folge \(s_n\) konvergiert mit Ordnung q, falls \(s_n\) gegen einen Wert s konvergiert und ein c \textgreater 0 existiert, sodass
    \[|s_{n+1} - s| \leqslant c|s_n - s|^q \text{ für alle } n \in \mathbb{N}\]
    Falls
    \[q = \lim_{n \to \infty} \frac{\log|{\frac{s_{k+1} - s_k}{s_k - s_{k-1}}|}}{\log{|\frac{s_k - s_{k-1}}{s_{k-1} - s_{k-2}}|}} \]
    existiert, so heißt dieser Grenzwert exakte q-Ordnung.
    Für \(q = 2\) heißt die Folge quadaratisch konvergent, für \(q = 3 \) kubisch usw. \\
    Eine exakte q-Ordnung größer 1 bedeutet, dass sich die Anzahl der genauen Dezimalstellen mit jeder Iteration ver-q-facht.
\end{definition}

\begin{definition}[Arithmetisch-Geometrisches Mittel]
    Seien \(a, b \in \mathbb{R}, a,b > 0\). Definiere 
    \[a_0 = a, \;\;\; b_0 = b\]
    \[a_{n+1} = \frac{a_n + b_n}{2}, \;\;\; b_{n+1} = \sqrt{a_n b_n}\]
    Dann heißt \(\lim_{n \to \infty}a_n = \lim_{n \to \infty} b_n =: M(a,b) \) das arithmetisch-geometrische Mittel von a und b.
    Dass die beiden Grenzwerte konvergieren, und dass sie gegen denselben Grenzwert konvergieren lässt sich recht einfach zeigen und kann 
    hier \cite{AGM} nachgelesen werden.
\end{definition}

\begin{definition}[Lineare Stichproben einer Folge]
    Für eine Folge \(S_n\) und \(a, b \in \mathbb{R}\) sowie \(k \in
    \mathbb{N}\) heißt das \(k\)-Tupel
    \begin{equation}
        \left(S_{a + i \cdot \frac{b}{k}}\right)_{i \in \{0, \ldots, k\}}
    \end{equation}
    \(k\) lineare Stichproben der Folge über dem Intervall \([a, b]\).
\end{definition}

\subsection{Rechenaufwand von Computerarithmetik}

In diesen Bericht werden wir die Rechenkosten unserer Algorithmen analysieren.
Um dies zu erleichtern, definieren wir einige allgemeine Parameter wie folgt:

\begin{enumerate}
    \item \textbf{Basisoperation}:
          \begin{itemize}
              \item Die Addition von zwei \texttt{64-Bit-Integern} dient als
                    Basisoperation, da dies auf den meisten Prozessoren
                    typischerweise maximal 1 CPU-Zyklus dauert.
          \end{itemize}

    \item \textbf{Multiplikation von \texttt{64-Bit-Integern}}:
          \begin{itemize}
              \item Die Multiplikation von zwei \texttt{64-Bit-Integern} dauert
                    ungefähr 20 CPU-Zyklen \cite{64-bit-Multiplication}.
          \end{itemize}

    \item \textbf{Multiplikation von Ganzzahlen mit beliebiger Genauigkeit}:
          \begin{itemize}
              \item Die Multiplikation von zwei Ganzzahlen mit beliebiger
                    Genauigkeit und der Genauigkeit \(n\) erfordert höchstens
                    \(n^{\log_2(3)}\) Einziffern-Multiplikationen. Daher
                    betragen die Kosten in Basiseinheiten:
                    \[
                        \left\lceil n^{\log_2(3)} \cdot \frac{20}{64} \right\rceil
                    \]
          \end{itemize}

    \item \textbf{Addition von Ganzzahlen mit beliebiger Genauigkeit}:
          \begin{itemize}
              \item Die Addition von zwei Ganzzahlen mit beliebiger Genauigkeit
                    und der Genauigkeit \(n\) dauert ungefähr:
                    \[
                        \left\lceil \frac{n}{64} \right\rceil
                    \]
                    Basiseinheiten.
          \end{itemize}

    \item \textbf{Addition und Multiplikation von \texttt{Decimal}}:
          \begin{itemize}
              \item Die Rechenkosten Addition und Multiplikation von
                    \texttt{Decimal}-Zahlen sind proportional zu den
                    Basiseinheiten, die für ihre Gegenstücke mit beliebiger
                    Genauigkeit erforderlich sind. Die Kosten für das interne
                    Verschieben des Exponenten sind vernachlässigbar und können
                    unberücksichtigt bleiben.
          \end{itemize}
\end{enumerate}

\subsection{Wahl eines angemessenen Datentypen}

Für die präzise Berechnung von \(\pi\) ist die Wahl eines angemessenen
Datentyps von entscheidender Bedeutung. Datentypen wie \texttt{float32} oder
\texttt{float64} bieten eine begrenzte Genauigkeit, die für viele
wissenschaftliche Anwendungen ausreichend sein mag, aber für die exakte
Annäherung von \(\pi\) über \(n\) beliebige Dezimalstellen hinweg nicht
ausreicht.
% TODO: Können wie hier die float Darstellung aus der Vorlesung nutzen?
Diese Datentypen nutzen eine feste Anzahl von Bits für die Mantisse und den
Exponenten, was ihre Präzision einschränkt.

\texttt{float32} hat eine Genauigkeit von etwa 7 Dezimalstellen, während
\texttt{float64} etwa 16 Dezimalstellen bietet.
Um \(\pi\) jedoch auf eine exakte Anzahl von Stellen genau zu approximieren,
benötigt man eine höhere Präzision, die durch diese Datentypen nicht erreicht
werden kann.
Hier kommt der \texttt{Decimal}-Datentyp ins Spiel, der eine beliebig wählbare,
feste Mantissen Länge erlaubt.

Eine weitere benötigte Eigenschaft des \texttt{Decimal}-Datentyps ist die
Vermeidung von Rundungsfehlern, die bei den Standard-Gleitkomma-Datentypen
auftreten können. Diese Fehler summieren sich bei iterativen Berechnungen, wie
sie zur Annäherung von \(\pi\) notwendig sind, und können zu signifikanten
Abweichungen führen.

\subsection{Annäherungsalgorithmen}

\begin{approximation sequence}[Monte-Carlo-Simulation]~\\[12pt]
\begin{figure}[H]
    \centering
    \includesvg[width=0.5\textwidth]{figures/monte-carlo-pi.svg}
    \caption{Monte-Carlo-Approximation von \(\pi\)}
    \label{fig:monte-carlo-approximation}
\end{figure}

Das Verfahren der Monte-Carlo-Simulation\footnote{Der Name ist eine Anspielung
auf die Spielbank Monte-Carlo im gleichnamigen Stadtteil des Stadtstaates
Monaco.\cite{anderson:1986:metropolis}} beruht auf dem Gesetz der großen
Zahlen. Es ermöglicht die numerische Lösung von Problemen, die analytisch
schwer oder nur mit großem Aufwand zu lösen sind, durch die Ziehung von
Stichproben.

Um die Konstante \(\pi\) mit diesem Verfahren zu approximieren, betrachten wir
zunächst die Menge \(M \coloneq \{(x, y) \mid 0 \le x,y \le 1, x, y \in
\mathbb{R}\}\), also die Menge der Punkte innerhalb des Einheitsquadrats.

Wir betrachten nun die Wahrscheinlichkeit \(P(M)\), dass ein Punkt in \(M\)
innerhalb des ersten Quadranten des Einheitskreises liegt, also eine Distanz
zum Ursprung \(\le 1\) hat:

\begin{equation}
    P(M) = \frac{\text{Rote Punkte}}{\text{Gesamte Punkte}} = \frac{|\{(x, y) \in M \mid \sqrt{x^2 + y^2} \le 1\}|}{|M|} = \frac{\frac{\pi \cdot 1^2}{4}}{1} = \frac{\pi}{4}
\end{equation}

Für eine Folge von gleichmäßig verteilten Teilmengen
\begin{equation}
    U_1 \subset U_2 \subset \cdots \subset U_n \subset M
\end{equation}
folgt nach dem Gesetz der großen Zahlen, dass
\begin{equation}
    \lim_{n \to \infty} P(U_n) = P(M)
\end{equation}
gilt.

Somit lässt sich \(\pi\) approximieren, indem man immer größere und größere
gleichmäßig verteilte Teilmengen von \(M\) findet. Solche Mengen \(U_n\) können
beispielsweise durch das wiederholte Generieren von zwei Zufallszahlen zwischen
0 und 1 mittels eines einfachen Programms erzeugt werden.

Definieren wir beispielsweise die Teilmenge \(U_{1400} \coloneq \text{Rote
Punkte} \cup \text{Gesamte Punkte}\) als die Vereinigung der roten und gesamten
Punkte aus Abbildung \ref{fig:monte-carlo-approximation}, so folgt daraus:
\begin{equation}
    P(U_{1400}) = \frac{r}{n} \Rightarrow \pi \approx \frac{4 \cdot r}{n} = \frac{4 \cdot 1100}{1400} = 3.1429
\end{equation}
ist.

\end{approximation sequence}

\begin{approximation sequence}[Leibniz-Reihe]
Mit Fortschritten der Analysis wurde die Leibniz-Reihe für \(\pi\) bereits im 14. 
oder 15. Jahrhundert von indischen Mathematikern entdeckt, und später dann in 
Mitte des 17. Jahrhunderts unabhängig voneinander von Wilhelm Leibniz und James Gregory.
Sie lautet:
\[ \frac{\pi}{4} = \sum_{k=0}^{\infty} \frac{(-1)^k}{2k+1} \]
und basiert auf der Taylor-Reihe für den Arkustangens \(arctanx = x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} + \cdots \)
Für \(x = 1\) erhält man das obige Ergebnis.
Ein recht simpler Beweis für die Reihendarstellung von \(arctanx\) wird im folgenden aufgeführt: \cite{Leibniz}
\begin{equation} 
    \begin{split}
    arctanx & = \int_{0}^{x} \frac{1}{1+t^2}\,dt \\
    & = \int_{0}^{x}\sum_{k=0}^{n}(-1)^k t^{2k} + \frac{(-1)^{n+1}t^{2n+2}}{1+t^2}\,dt \\
    & = \sum_{k=0}^{n}(-1)^k \frac{x^{2k+1}}{2k+1} + (-1)^{n+1}\int_{0}^{x}\frac{t^{2n+2}}{1+t^2}\,dt
    \end{split}
    \end{equation}
Da aber für \(|x| \leqslant 1\) gilt: \[|\int_{0}^{x}\frac{t^{2n+2}}{1+t^2}\,dt| \leqslant |\int_{0}^{x}t^{2n+2}\,dt| = \frac{|x|^{2n+3}}{2n+3} \rightarrow 0 \text{ für } n \rightarrow \infty \]
folgt die obige Aussage
\[arctanx = \sum_{k=0}^{\infty}(-1)^k \frac{x^{2k+1}}{2k+1} = x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} + \cdots \] 
Und die Folge \(L_n := 4\sum_{k=0}^{n} \frac{(-1)^k}{2k+1} \) ist eine Approximationsfolge für \(\pi\).
Außerdem lässt sich mit Definition 3 leicht nachrechnen, dass die Leibniz-Reihe logarithmisch konvergiert. 
\end{approximation sequence}


\begin{approximation sequence}[Gauß-Legendre]
Der Gauß-Legendre Algorithmus ist benannt nach den beiden Mathematikern Carl Friedrich Gauß 
und Adrien-Marie Legendre, die inviduell Arbeit beigetragen haben, auf denen dieser Algorithmus basiert.
Dieser Algorithmus bedient sich vier verschiedener Folgen, sowie dem arithmetisch-geometrischen Mittels, 
wie in Definition 4, um \(\pi\) zu approximieren.
Im folgenden wird der Algorithmus aufgeführt: \cite{AGM-Gauß-Legendre}
Die Startwerte lauten wie folgt:
\[a_0 = 1, \;\;\; b_0 = \frac{1}{\sqrt{2}}, \;\;\; t_0 = \frac{1}{4}, \;\;\; p_0 = 1 \]
Mit den Iterationsvorschriften:
\[a_{n+1} = \frac{a_n + b_n}{2} \] 
\[b_{n+1} = \sqrt{a_nb_n} \]
\[t_{n+1} = t_n - p_n(a_n - a_{n+1})^2 \]
\[p_{n+1} = 2p_n \] 

Dann gilt für \(g_n := \frac{(a_{n+1} + b_{n+1})^2}{4t_{n+1}}\):
\[\lim_{n \to \infty}g_n = \pi \] 

Als mathematische Grundlagen \cite{Eugene-Salamin} für diesen Algorithmus dienen zum einen das arithmetische-geometrische Mittel welches 
für \(a_0 = 1 \text{ und } b_0 = \cos(\phi)\) gegen \(\frac{\pi}{2K(\sin(\phi))}\) konvergiert, wobei
\[K(k) = \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2sin^2(\theta)}}d\theta \] das elliptische Integral erster Art ist und zum anderen
der Fakt, dass für \(c_0 = \sin(\phi), \\ c_{i+1} = a_i - a_{i+1}\)
\[ \sum_{i= 0}^{\infty} 2^{i-1} c_{i}^{2} = 1 - \frac{E(sin(\phi))}{K(sin(\phi))} \]
wobei \[E(k) = \int_{0}^{\frac{\pi}{2}}\sqrt{1-k^2sin^2(\theta)}d\theta\] das elliptische Integral zweiter Art ist,
und einer Identität, die Legendre bewies:
\[K(\cos(\theta))E(\sin(\theta)) + K(\sin(\theta))E(cos(\theta)) - K(\cos(\theta))K(\sin(\theta)) = \frac{\pi}{2} \] 
Der Beweis, dass der oben genannte Algorithmus tatsächlich gegen \(\pi\) konvergiert und dazu noch quadratisch konvergiert, ist zu lang 
für diese Arbeit, kann jedoch mit der Integralrechnung durchgeführt werden und hier \cite{Gauß-Legendre} nachgelesen werden. \\
Die Laufzeit für den Gauß-Legendre Algorithmus für eine gegebene Präzision n beträgt \(\mathcal{O}(\frac{15}{2}M(n)log_2(n))\),
wobei \(M(n)\) die Zeit ist, um floating-point Zahlen mit n-Bit Bruchteil zu multiplizieren. \cite{AGM-Gauß-Legendre}
Für den Datentyp Decimal gilt \(M(n) = n^{\log_{2}(3)}\) mit der Karatsuba Multiplikation für Decimals \cite{Decimal-Multiplication}.
Also liegt die Laufzeit insgesamt in \(\mathcal{O}(\frac{15}{2}n^{\log_{2}(3)}log_2(n))\).
\end{approximation sequence}

\begin{approximation sequence}[Chudnovsky-Algorithmus]
Der Chudnovsky Algorithmus, benannt nach seinen Entdeckern den Chudnovsky Brüdern, ist eine schnelle Methode um \(\pi\) zu berechnen. 
Der Algorithmus basiert auf folgender Reihe\cite{Chudnovsky}:
\[\frac{1}{\pi} = \frac{1}{426880\sqrt{10005}}\sum_{k = 0}^{\infty}\frac{(-1)^k(6k)!(545140134k+13591409)}{(3k)!(k!)^3(640320)^{3k}}\]
Die Laufzeitkomplexität liegt in \(\mathcal{O}(n(logn)^3)\) \cite{Runtime-Chudnovsky} und der Algorithmus produziert mit jeder Iteration durchschnittlich
14.81 weitere korrekte Dezimalstellen \cite{Nachkommastellen-Chudnovsky}, was ihn zu einem sehr guten Approximationsalgorithmus für \(\pi\) macht.
Auf einen Beweis dieser Gleichung wird verzichtet und kann hier \cite{Chudnovsky-Proof} nachgelesen werden.
Dieser Algorithmus wurde für viele Weltrekorde, im Bereich \(\pi\) approximieren benutzt, jedoch wurde in diesem Fall eine Optimierung 
des Algorithmus', das sogenannte Binary-Splitting verwendet\cite{Chudnovsky}.Während es interessant wäre auch diese zu untersuchen, haben wir uns aber dazu
entschieden diese wegzulassen, da sie über den Rahmen dieser Arbeit hinausgeht.  

\end{approximation sequence}

\section{Experimente}

\subsection{Laufzeitverhalten und Rechenaufwand}

Das erste Auswahlkriterium, das wir uns für die Folgen anschauen werden, ist
die Laufzeit in Relation zum Rechenaufwand. In diesem Experiment wird für jede
Folge die Laufzeit berechnet, die benötigt wird, um \(n\) Dezimalstellen
korrekt zu approximieren. Zusätzlich wird die relative Laufzeit für eine
einzelne Dezimalstelle berechnet. Die Mantissenlänge \(m\) wird hierbei
festgelegt und hängt von der Anzahl der zu approximierenden Dezimalstellen ab.

\begin{figure}[htp]
    \centering
    \subfloat[\centering \(n = 2^{11}\)]{{\includesvg[width=0.45\textwidth]{figures/runtime-only-fast.svg} }}
    \hspace{5cm}
    \subfloat[\centering \(n = 7\)]{{\includesvg[width=0.45\textwidth]{figures/runtime-only-slow.svg} }}
    \caption{%
        Laufzeit in Abhängigkeit von der Anzahl der zu approximierenden Dezimalstellen mit \(n\) (Parameter \texttt{--digits} im Skript) und \(k = 20\) (Parameter \texttt{--samples} im Skript).
    }
\end{figure}

\begin{observation}
    Die Analyse zeigt, dass die Laufzeit stark von der Anzahl der zu approximierenden Dezimalstellen abhängt. Für größere Werte von \(n\) (z. B. \(n = 2^{11}\)) steigt die Laufzeit erheblich an, während für kleinere Werte von \(n\) (z. B. \(n = 7\)) die Laufzeit relativ gering bleibt. Diese Ergebnisse deuten darauf hin, dass die Wahl des Parameters \(n\) entscheidend für die Effizienz der Approximationsfolge ist.
\end{observation}

\subsection{Konvergenzanalyse}

Ein sehr relevanter Faktor für die Auswahl einer Approximationsfolge ist deren
Konvergenz verhalten.

In diesem Experiment werden \(k\) lineare Stichproben der Folgen für das
Intervall \([0, b]\) für eine festgesetzte Mantissenlänge \(m \in \mathbb{N}\)
genommen. Ziel ist es, experimentell eine Darstellung der
Konvergenzgeschwindigkeit zu approximieren. Eine höhere
Konvergenzgeschwindigkeit impliziert hier eine bessere Approximationsfolge.

\begin{figure}[H]
    \centering
    \subfloat[\centering \(b = 10^2\)]{{\includesvg[width=0.5\textwidth]{figures/convergence-only-fast.svg} }}
    \subfloat[\centering \(b = 10^6\)]{{\includesvg[width=0.5\textwidth]{figures/convergence-only-slow.svg} }}
    \caption{%
        Anzahl an korrekt approximierten Dezimalstellen in Abhängigkeit von der
        Position innerhalb der Sequenz mit \(b\) (Parameter \texttt{--stop} im
        Skript), \(k = 20\) (Parameter \texttt{--samples} im Skript) und \(m =
        2^9\) (Parameter \texttt{--precision} im Skript). }
        \label{fig:convergence-analysis}
\end{figure}

\begin{observation}
    Dieses Experiment dient dazu den Rechenaufwand mit der erzielten Präzision zu vergleichen. Dazu haben wir zum einen die Leibniz-Reihe mit 
    der Monte-Carlo Simulation verglichen, und zum anderen den Gauß-Legendre Algorithmus mit dem Chudnovsky Algorithmus.
    Grund für diese Aufteilung ist, dass die beiden zuerst aufgeführten Algorithmen auch für sehr viele Iterationen eine geringe Präzision aufweisen.
    
    Zuerst vergleichen wir die Leibniz-Reihe mit der Monte-Carlo Simulation. Beide Algorithmen müssen erst ein mal mehrere Iterationen ausführen um eine
    Dezimalstelle korrekt zu approximieren, ca. 30 für die Leibniz-Reihe und fast 200 für die Monte-Carlo Simulation. Beide approximieren dann nach mehreren 
    hundert Iteration zwar zwei korrekte Dezimalstellen, fallen dann jedoch wieder auf eine herab.
    Während die Leibniz-Reihe hiernach, ab ca. 400 Iteration kontinuierlich, wenn auch langsam, an Präzision gewinnt, so schwankt die Monte-Carlo Simulation, was korrekt approximierte 
    Dezimalstellen angeht.
    Über 10,000,000 Millionen Iterationen schafft es die Leibniz-Reihe \(\pi\) auf insgesamt sechs korrekte Dezimalstellen zu approximieren, während die Monte-Carlo Simulation nur auf zwei kommt.
    
    Als nächstes vergleichen wir den Gauß-Legendre Algorithmus mit dem Chudnovsky Algorithmus. Beide Algorithmen gewinnen sehr schnell an Genauigkeit und während Chudnovsky für die ersten drei
    Iteration genauer ist als Gauß-Legendre, so wird für vier oder mehr Iterationen der letztere Algorithmus deutlich besser. So hat der Gauß-Legendre Algorithmus die eingestellte Präzision von 512 Dezimalstellen schon nach
    acht Iterationen erreicht, während der Chudnovsky Algorithmus hierfür etwas mehr als 30 Iterationen benötigt. Was auch zu beobachten ist, in diesem doppelt logarithmisch skaliertem Plot, ist dass der Chudnovsky Algorithmus
    ungefähr linear ansteigt, während der Gauß-Legendre Algorithmus etwas schneller als linear ansteigt.
\end{observation}

\subsection{Speicherbedarf}

Ein weiterer relevanter Faktor für die Auswahl eines Approximationsalgorithmus
ist der Arbeitsspeicherbedarf während der Laufzeit. Benötigt ein Algorithmus
beispielsweise einen quadratisch zur Anzahl der zu approximierenden
Dezimalstellen wachsenden Speicherbedarf, so kann ein herkömmlicher Computer
mit 16 GB Arbeitsspeicher eine maximale Anzahl von etwa 125.000 Dezimalstellen
berechnen, unabhängig von der Rechenzeit.

In diesem Experiment wird der Speicherbedarf der verschiedenen
Annäherungsfolgen in Abhängigkeit von der zu approximierenden
Dezimalstellenanzahl analysiert. Jede Sequenz wird auf dem Intervall \((0,
n)\), wobei \(n \in \mathbb{N}\) die Anzahl der zu approximierenden
Dezimalstellen ist, \(k \in \mathbb{N}\)-mal mit einer Mantissenlänge von \(\left(i
\frac{n}{k}\right)_{i \in \{0, \ldots, k\}}\) zu einer festen Position
ausgewertet und ihr benötigter Speicherbedarf wird festgehalten.

\begin{figure}[H]
    \centering
    \includesvg[width=0.6\textwidth]{figures/memory-usage.svg}
    \caption{
        Speicherbedarf der verschiedenen Pi-Annäherungsfolgen in Abhängigkeit
        von der Anzahl der Dezimalstellen mit \(n = 512\) (Parameter
        \texttt{--digits} im Skript) und \(k = 20\) (Parameter
        \texttt{--samples} im Skript).
    }
    \label{fig:memory-usage}
\end{figure}

\begin{observation}
    Die Analyse zeigt, dass der Speicherbedarf mit zunehmender Anzahl der
    Dezimalstellen für alle Sequenzen linear ansteigt. Dabei ist der
    Speicherbedarf der Gauss-Legendre-Sequenz am höchsten, gefolgt von der
    Chudnovsky-Sequenz. Die Leibniz- und Monte-Carlo-Sequenzen zeigen einen
    vergleichsweise geringeren Anstieg des Speicherbedarfs, was mit deren
    relativ wenigen internen Variablen zusammenhängt. Besonders hervorzuheben
    ist, dass die Monte-Carlo-Sequenz den geringsten Speicherbedarf aufweist,
    was auf ihre probabilistische Natur und geringere Genauigkeit bei höherer
    Präzision hinweist.
\end{observation}

\section{Auswertung}

\section{Zusammenfassung}
Welche Erkenntnisse haben wir also mit unseren Experimenten gewonnen? Zum einen stimmen die Experimente mit der Theorie überein, beispielsweise die 
langsame Konvergenz der Monte-Carlo Simulation und der Leibniz-Reihe waren in dem Experimentabschnitt "Konvergenzanalyse" deutlich zu sehen. 
Außerdem waren auch konkrete Eigenschaften des Gauß-Legendre Algorithmus, namentlich die quadratische Konvergenz, also die Verdoppelung an korrekten Dezimalstellen 
mit jeder Iteration, sowie die Tatsache, dass der Chudnovsky Algorithmus mit jeder Iteration grob 15 weitere korrekte Dezimalstellen produziert, auch im Graphen zu erkennen.

Wie wir auch in unserer Analyse zum Speicherbedarfs gesehen haben, steigt dieser linear für jeden Algorithmus mit zunehmenden korrekten Dezimalstellen an, und ist für den Gauß-Legendre Algorithmus, aufgrund seiner vier Folgen,
deren Folgenglieder berechnet werden müssen, am größten. Gefolgt wird dieser vom Chudnovsky Algorithmus, dann der Monte-Carlo Simulation und zuletzt der Leibniz-Reihe, die am wenigsten Speicherplatz pro korrekter Dezimalstelle 
verbraucht.

\printbibliography

\end{document}
