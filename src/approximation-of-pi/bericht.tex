\documentclass{scrartcl}
\usepackage{scrhack}
\subject{Bericht}
\titlehead{%
  Humboldt-Universität zu Berlin\\
  Mathematisch-Naturwissenschaftliche Fakultät\\
  Institut für Mathematik
}
\title{Annäherungen von \(\pi\)}
\author{
  Eingereicht von M. van Straten und P. Merz
}
\date{\today} % This will be set to the data of the last modification by the build-system 

\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{svg}

\addbibresource{./bericht.bib}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{approximation sequence}{Annäherungsfolge}
\newtheorem{observation}{Beobachtung}

\parskip = \baselineskip
\parindent = 0pt

\begin{document}
\maketitle
\cleardoublepage{}
\tableofcontents
\cleardoublepage{}

\section{Einleitung}

Die Konstante \(\pi\) hat eine natürliche Definition in der euklidischen
Geometrie als das Verhältnis zwischen dem Umfang und dem Durchmesser eines
Kreises. Sie ist von fundamentaler Bedeutung in vielen Bereichen der Mathematik
und Physik. Beispielsweise tritt \(\pi\) im Gaußschen Integral auf, bei den
komplexen Einheitswurzeln und in der Cauchy-Verteilung in der
Wahrscheinlichkeitstheorie. Die Relevanz von \(\pi\) erstreckt sich dabei über
zahlreiche Disziplinen hinweg, was sie zu einer der wichtigsten Konstanten in
der Wissenschaft macht.

\subsection{Historie}
Historisch gesehen haben sich Mathematiker stets bemüht, \(\pi\) immer genauer
zu approximieren.
In der Antike reichte die Genauigkeit solcher Annäherungen oft nur bis zur
zweiten Nachkommastelle.
Signifikante Fortschritte wurden im 14.
Jahrhundert durch Madhava von Sangamagrama erzielt, der Methoden entwickelte,
um \(\pi\) bis auf elf und später dreizehn Stellen genau zu berechnen.
In der chinesischen Mathematik wurde im 5.
Jahrhundert eine Genauigkeit von etwa sieben Dezimalstellen erreicht.
Ende des 20. Jahrhunderts hatte sich diese Präzision auf 2 Millionen Dezimalstellen, erreicht durch Jean Guilloud im Jahr 1981, erhöht und
ist innerhalb weniger Jahre bis 1988 auf das zehnfache, ca. 200 Millionen Nachkommastellen, angestiegen.
Bis zum heutigen Jahr gab es viele Fortschritte in diesem Fachgebiet, so wurde \(\pi\) 2009 von Fabrice Bellard auf 2,699,999,990,000 Dezimalstellen approximiert und
die neueste Entwicklung ist von März 2024, als Jordan Ranous, Kevin O’Brien und Brian Beeler es sogar schafften \(\pi\) auf 105,000,000,000,000 Dezimalstellen zu approximieren. \cite{Chronology}

\subsection{Motivation}
Diese historischen Entwicklungen zeigen die Bedeutung und den anhaltenden
wissenschaftlichen Fortschritt in der Annäherung von \(\pi\). Die genaue
Berechnung von \(\pi\) bleibt nicht nur eine mathematische Herausforderung,
sondern ist auch für viele praktische Anwendungen relevant, etwa in der
Ingenieurwissenschaft, der Physik und der Informatik.

Ziel dieses Berichts ist es, verschiedene mathematische Annäherungsmethoden zur
Berechnung von \(\pi\) zu untersuchen und miteinander zu vergleichen.
Neben der bereits bekannten Leibnizreihe werden zwei weitere Verfahren
implementiert und analysiert.
Diese Verfahren werden hinsichtlich ihrer Laufzeit, ihres Speicherbedarfs,
ihres Konvergenzverhaltens und des Verhältnisses von Rechenaufwand zu
Approximationsgenauigkeit sowohl analytisch als auch experimentell verglichen.


\section{Theorie}

\subsection{Einführung und Einordnung von Fachbegriffen}

\begin{definition}[Annäherungsfolge]
    Sei \((a_n)_{n \in \mathbb{N}}\) eine Folge reeller Zahlen und \(P \in \mathbb{R}\), sodass
    \[\lim_{n \to \infty} a_n = P \] gilt. Die Folge \(a_n\) heißt dann Annäherungsfolge für den Punkt \(P\)

\end{definition}

\begin{definition}[Uniforme Teilmenge]
    Eine uniforme Teilmenge \(U\) einer Menge \(M\) ist eine Teilmenge, in der
    jedes Element mit gleicher Wahrscheinlichkeit ausgewählt wird.
    Dies bedeutet, dass für jedes \(x \in M\) die Wahrscheinlichkeit \(P(x \in U)\)
    konstant ist.
\end{definition}

\begin{definition}[Konvergenzgeschwindigkeit \cite{Konvergenzgeschwindigkeit}]
    Sei \((s_n)_{n \in \mathbb{N}}\) eine Approximationsfolge mit Grenzwert s, wobei O. B. d. A. alle Folgeglieder paarweise verschieden und ungleich dem Grenzwert selbst sind.
    Dann konvergiert \(s_n\) linear, falls
    \[\limsup_{n \to \infty} c_n < 1 \text{ mit } c_n := \frac{|s_{n+1}-s|}{|s_k-s|} \]
    Falls c = 1, so konvergiert die Folge sublinear.
    Gilt zusätzlich 
    \[\lim_{n \to \infty} \frac{|s_{n+2}-s_{n+1}|}{|s_{n+1}-s_n|} = 1\]
    so heißt die Folge logarithmisch konvergent.
    c wird öfter auch als Konvergenzrate bezeichnet, denn je kleiner c, desto schneller konvergiert die Folge gegen ihren Grenzwert, 
    d.h. für eine gewünschte Präzision werden weniger Iterationen benötigt.
    Zudem lässt sich die Konvergenz der Ordnung q wie folgt definieren:
    Eine Folge \(s_n\) konvergiert mit Ordnung q, falls \(s_n\) gegen einen Wert s konvergiert und ein c \textgreater 0 existiert, sodass
    \[|s_{n+1} - s| \leqslant c|s_n - s|^q \text{ für alle } n \in \mathbb{N}\]
    Falls
    \[q = \lim_{n \to \infty} \frac{\log|{\frac{s_{k+1} - s_k}{s_k - s_{k-1}}|}}{\log{|\frac{s_k - s_{k-1}}{s_{k-1} - s_{k-2}}|}} \]
    existiert, so heißt dieser Grenzwert exakte q-Ordnung.
    Für \(q = 2\) heißt die Folge quadaratisch konvergent, für \(q = 3 \) kubisch usw. \\
    Eine exakte q-Ordnung größer 1 bedeutet, dass sich die Anzahl der genauen Dezimalstellen mit jeder Iteration ver-q-facht.
\end{definition}

\begin{definition}[Arithmetisch-Geometrisches Mittel]
    Seien \(a, b \in \mathbb{R}, a,b > 0\). Definiere 
    \[a_0 = a, \;\;\; b_0 = b\]
    \[a_{n+1} = \frac{a_n + b_n}{2}, \;\;\; b_{n+1} = \sqrt{a_n b_n}\]
    Dann heißt \(\lim_{n \to \infty}a_n = \lim_{n \to \infty} b_n =: M(a,b) \) das arithmetisch-geometrische Mittel von a und b.
    Dass die beiden Grenzwerte konvergieren, und dass sie gegen denselben Grenzwert konvergieren lässt sich recht einfach zeigen und kann 
    hier \cite{AGM} nachgelesen werden.
\end{definition}


\subsection{Wahl eines angemessenen Datentypen}

Für die präzise Berechnung von \(\pi\) ist die Wahl eines angemessenen
Datentyps von entscheidender Bedeutung. Datentypen wie \texttt{float32} oder
\texttt{float64} bieten eine begrenzte Genauigkeit, die für viele
wissenschaftliche Anwendungen ausreichend sein mag, aber für die exakte
Annäherung von \(\pi\) über \(n\) beliebige Dezimalstellen hinweg nicht
ausreicht.
% TODO: Können wie hier die float Darstellung aus der Vorlesung nutzen?
Diese Datentypen nutzen eine feste Anzahl von Bits für die Mantisse und den
Exponenten, was ihre Präzision einschränkt.

\texttt{float32} hat eine Genauigkeit von etwa 7 Dezimalstellen, während
\texttt{float64} etwa 16 Dezimalstellen bietet.
Um \(\pi\) jedoch auf eine exakte Anzahl von Stellen genau zu approximieren,
benötigt man eine höhere Präzision, die durch diese Datentypen nicht erreicht
werden kann.
Hier kommt der \texttt{Decimal}-Datentyp ins Spiel, der eine beliebig wählbare,
feste Mantissen Länge erlaubt.

Eine weitere benötigte Eigenschaft des \texttt{Decimal}-Datentyps ist die
Vermeidung von Rundungsfehlern, die bei den Standard-Gleitkomma-Datentypen
auftreten können Diese Fehler summieren sich bei iterativen Berechnungen, wie
sie zur Annäherung von \(\pi\) notwendig sind, und können zu signifikanten
Abweichungen führen.

\subsection{Annäherungsalgorithmen}

\begin{approximation sequence}[Monte-Carlo-Simulation]~\\[12pt]
\begin{figure}[H]
    \centering
    \includesvg[width=0.5\textwidth]{figures/monte-carlo-pi.svg}
    \caption{Monte-Carlo-Approximation von \(\pi\)}
    \label{fig:monte-carlo-approximation}
\end{figure}

Das Verfahren der Monte-Carlo-Simulation\footnote{Der Name ist eine Anspielung
auf die Spielbank Monte-Carlo im gleichnamigen Stadtteil des Stadtstaates
Monaco.\cite{anderson:1986:metropolis}} beruht auf dem Gesetz der großen
Zahlen. Es ermöglicht die numerische Lösung von Problemen, die analytisch
schwer oder nur mit großem Aufwand zu lösen sind, durch die Ziehung von
Stichproben.

Um die Konstante \(\pi\) mit diesem Verfahren zu approximieren, betrachten wir
zunächst die Menge \(M \coloneq \{(x, y) \mid 0 \le x,y \le 1, x, y \in
\mathbb{R}\}\), also die Menge der Punkte innerhalb des Einheitsquadrats.

Wir betrachten nun die Wahrscheinlichkeit \(P(M)\), dass ein Punkt in \(M\)
innerhalb des ersten Quadranten des Einheitskreises liegt, also eine Distanz
zum Ursprung \(\le 1\) hat:

\begin{equation}
    P(M) = \frac{\text{Rote Punkte}}{\text{Gesamte Punkte}} = \frac{|\{(x, y) \in M \mid \sqrt{x^2 + y^2} \le 1\}|}{|M|} = \frac{\frac{\pi \cdot 1^2}{4}}{1} = \frac{\pi}{4}
\end{equation}

Für eine Folge von gleichmäßig verteilten Teilmengen
\begin{equation}
    U_1 \subset U_2 \subset \cdots \subset U_n \subset M
\end{equation}
folgt nach dem Gesetz der großen Zahlen, dass
\begin{equation}
    \lim_{n \to \infty} P(U_n) = P(M)
\end{equation}
gilt.

Somit lässt sich \(\pi\) approximieren, indem man immer größere und größere
gleichmäßig verteilte Teilmengen von \(M\) findet. Solche Mengen \(U_n\) können
beispielsweise durch das wiederholte Generieren von zwei Zufallszahlen zwischen
0 und 1 mittels eines einfachen Programms erzeugt werden.

Definieren wir beispielsweise die Teilmenge \(U_{1400} \coloneq \text{Rote
Punkte} \cup \text{Gesamte Punkte}\) als die Vereinigung der roten und gesamten
Punkte aus Abbildung \ref{fig:monte-carlo-approximation}, so folgt daraus:
\begin{equation}
    P(U_{1400}) = \frac{r}{n} \Rightarrow \pi \approx \frac{4 \cdot r}{n} = \frac{4 \cdot 1100}{1400} = 3.1429
\end{equation}
ist.

\end{approximation sequence}

\begin{approximation sequence}[Leibniz-Formel]
Mit Fortschritten der Analysis wurde die Leibniz Reihe für \(\pi\) bereits im 14. 
oder 15. Jahrhundert von indischen Mathematikern entdeckt, und später dann in 
Mitte des 17. Jahrhunderts unabhängig voneinander von Wilhelm Leibniz und James Gregory.
Sie lautet:
\[ \frac{\pi}{4} = \sum_{k=0}^{\infty} \frac{(-1)^k}{2k+1} \]
und basiert auf der Taylor-Reihe für den Arkustangens \(arctanx = x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} + \cdots \)
Für \(x = 1\) erhält man das obige Ergebnis.
Ein recht simpler Beweis für die Reihendarstellung von \(arctanx\) wird im folgenden aufgeführt: \cite{Leibniz}
\begin{equation} 
    \begin{split}
    arctanx & = \int_{0}^{x} \frac{1}{1+t^2}\,dt \\
    & = \int_{0}^{x}\sum_{k=0}^{n}(-1)^k t^{2k} + \frac{(-1)^{n+1}t^{2n+2}}{1+t^2}\,dt \\
    & = \sum_{k=0}^{n}(-1)^k \frac{x^{2k+1}}{2k+1} + (-1)^{n+1}\int_{0}^{x}\frac{t^{2n+2}}{1+t^2}\,dt
    \end{split}
    \end{equation}
Da aber für \(|x| \leqslant 1\) gilt: \[|\int_{0}^{x}\frac{t^{2n+2}}{1+t^2}\,dt| \leqslant |\int_{0}^{x}t^{2n+2}\,dt| = \frac{|x|^{2n+3}}{2n+3} \rightarrow 0 \text{ für } n \rightarrow \infty \]
folgt die obige Aussage
\[arctanx = \sum_{k=0}^{\infty}(-1)^k \frac{x^{2k+1}}{2k+1} = x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} + \cdots \] 
Und die Folge \(L_n := 4\sum_{k=0}^{n} \frac{(-1)^k}{2k+1} \) ist eine Approximationsfolge für \(\pi\).
Außerdem lässt sich mit Definition 3 leicht nachrechnen, dass die Leibnizreihe logarithmisch konvergiert. 
\end{approximation sequence}


\begin{approximation sequence}[Gauß-Legendre]
Der Gauß-Legendre Algorithmus, benannt nach den beiden Mathematikern Carl Friedrich Gauß 
und Adrien-Marie Legendre, die inviduell Arbeit beigetragen haben, auf denen dieser Algorithmus basiert.
Dieser Algorithmus bedient sich vier verschiedener Folgen, sowie dem arithmetisch-geometrischen Mittels, 
wie in Definition 4, um \(\pi\) zu approximieren.
Im folgenden wird der Algorithmus aufgeführt: \cite{AGM-Gauß-Legendre}
Die Startwerte lauten wie folgt:
\[a_0 = 1, \;\;\; b_0 = \frac{1}{\sqrt{2}}, \;\;\; t_0 = \frac{1}{4}, \;\;\; p_0 = 1 \]
Mit den Iterationsvorschriften:
\[a_{n+1} = \frac{a_n + b_n}{2} \] 
\[b_{n+1} = \sqrt{a_nb_n} \]
\[t_{n+1} = t_n - p_n(a_n - a_{n+1})^2 \]
\[p_{n+1} = 2p_n \] 

Dann gilt für \(g_n := \frac{(a_{n+1} + b_{n+1})^2}{4t_{n+1}}\):
\[\lim_{n \to \infty}g_n = \pi \] 

Als mathematische Grundlagen \cite{Eugene-Salamin} für diesen Algorithmus dienen zum einen das arithmetische-geometrische Mittel welches 
für \(a_0 = 1 \text{ und } b_0 = \cos(\phi)\) gegen \(\frac{\pi}{2K(\sin(\phi))}\) konvergiert, wobei
\[K(k) = \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2sin^2(\theta)}}d\theta \] das elliptische Integral erster Art ist und zum anderen
der Fakt, dass für \(c_0 = \sin(\phi), \\ c_{i+1} = a_i - a_{i+1}\)
\[ \sum_{i= 0}^{\infty} 2^{i-1} c_{i}^{2} = 1 - \frac{E(sin(\phi))}{K(sin(\phi))} \]
wobei \[E(k) = \int_{0}^{\frac{\pi}{2}}\sqrt{1-k^2sin^2(\theta)}d\theta\] das elliptische Integral zweiter Art ist,
und einer Identität, die Legendre bewies:
\[K(\cos(\theta))E(\sin(\theta)) + K(\sin(\theta))E(cos(\theta)) - K(\cos(\theta))K(\sin(\theta)) = \frac{\pi}{2} \] 
Der Beweis, dass der oben genannte Algorithmus tatsächlich gegen \(\pi\) konvergiert und dazu noch quadratisch konvergiert, ist zu lang 
für diese Arbeit, kann jedoch mit der Integralrechnung durchgeführt werden und hier \cite{Gauß-Legendre} nachgelesen werden. \\
Die Laufzeit für den Gauß-Legendre Algorithmus für eine gegebene Präzision n beträgt \(\mathcal{O}(\frac{15}{2}M(n)log_2(n))\),
wobei \(M(n)\) die Zeit ist, um floating-point Zahlen mit n-Bit Bruchteil zu multiplizieren. \cite{AGM-Gauß-Legendre}
Für den Datentyp Decimal gilt \(M(n) = n^2\) mit der Karatsuba Multiplikation für Decimals \cite{Decimal-Multiplication}.
Also liegt die Laufzeit insgesamt in \(\mathcal{O}(\frac{15}{2}n^2log_2(n))\).
\end{approximation sequence}

\begin{approximation sequence}[Chudnovsky-Algorithmus]

\end{approximation sequence}

\section{Experimente}

\subsection{Laufzeitverhalten und Rechenaufwand}

\begin{observation}

\end{observation}

\subsection{Konvergenzanalyse}

In diesem Experiment wird die Konvergenz verschiedener Pi-Annäherungsfolgen
analysiert. Ziel ist es, die Anzahl der korrekt approximierten Stellen von Pi
in Abhängigkeit von der Position innerhalb der Sequenz auf einer
logarithmischen Skala darzustellen. Die verwendeten Sequenzen umfassen Leibniz,
Monte Carlo, Gauss-Legendre und Chudnovsky.

Um die Ergebnisse zu reproduzieren, können die folgenden Parameter verwendet
werden:
\begin{verbatim}
approximation-of-pi convergence --precision 50 --stop 4
\end{verbatim}

\begin{figure}[H]
    \centering
    \includesvg[width=0.6\textwidth]{figures/convergence.svg}
    \caption{Konvergenzanalyse der verschiedenen Pi-Annäherungsmethoden in Abhängigkeit von der Position innerhalb der Sequenz (logarithmische Skala).}
    \label{fig:convergence-analysis}
\end{figure}

\begin{observation}
    Dieses Experiment dient dazu den Rechenaufwand mit der erzielten Präzision zu vergleichen. Dazu haben wir zum einen die Leibniz-Reihe mit 
    der Monte-Carlo Simulation verglichen, und zum anderen den Gauß-Legendre Algorithmus mit dem Chudnovsky Algorithmus.
    Grund für diese Aufteilung ist, dass die beiden zuerst aufgeführten Algorithmen auch für sehr viele Iterationen eine geringe Präzision aufweisen.
    
    Zuerst vergleichen wir die Leibniz-Reihe mit der Monte-Carlo Simulation. Beide Algorithmen müssen erst ein mal mehrere Iterationen ausführen um eine
    Dezimalstelle korrekt zu approximieren, ca. 30 für die Leibniz-Reihe und fast 200 für die Monte-Carlo Simulation. Beide approximieren dann nach mehreren 
    hundert Iteration zwar zwei korrekte Dezimalstellen, fallen dann jedoch wieder auf eine herab.
    Während die Leibniz-Reihe hiernach, ab ca. 400 Iteration kontinuierlich, wenn auch langsam, an Präzision gewinnt, so schwankt die Monte-Carlo Simulation, was korrekt approximierte 
    Dezimalstellen angeht.
    Über 10,000,000 Millionen Iterationen schafft es die Leibniz-Reihe \(\pi\) auf insgesamt sechs korrekte Dezimalstellen zu approximieren, während die Monte-Carlo Simulation nur auf zwei kommt.
    
    Als nächstes vergleichen wir den Gauß-Legendre Algorithmus mit dem Chudnovsky Algorithmus. Beide Algorithmen gewinnen sehr schnell an Genauigkeit und während Chudnovsky für die ersten drei
    Iteration genauer ist als Gauß-Legendre, so wird für vier oder mehr Iterationen der letztere Algorithmus deutlich besser. So hat der Gauß-Legendre Algorithmus die eingestellte Präzision von 512 Dezimalstellen schon nach
    acht Iterationen erreicht, während der Chudnovsky Algorithmus hierfür etwas mehr als 30 Iterationen benötigt. Was auch zu beobachten ist, in diesem doppelt logarithmisch skaliertem Plot, ist dass der Chudnovsky Algorithmus
    ungefähr linear ansteigt, während der Gauß-Legendre Algorithmus etwas schneller als linear ansteigt.
\end{observation}

\subsection{Speicherbedarf}

Ein weiterer relevanter Faktor für die Auswahl eines Approximationsalgorithmus
ist der Arbeitsspeicherbedarf während der Laufzeit. Benötigt ein Algorithmus
beispielsweise einen quadratisch zur Anzahl der zu approximierenden
Dezimalstellen wachsenden Speicherbedarf, so kann ein herkömmlicher Computer
mit 16 GB Arbeitsspeicher eine maximale Anzahl von etwa 125.000 Dezimalstellen
berechnen, unabhängig von der Rechenzeit.

In diesem Experiment wird der Speicherbedarf der verschiedenen
Annäherungsfolgen in Abhängigkeit von der zu approximierenden
Dezimalstellenanzahl analysiert. Jede Sequenz wird auf dem Intervall \((0,
n)\), wobei \(n \in \mathbb{N}\) die Anzahl der zu approximierenden
Dezimalstellen ist, \(k \in \mathbb{N}\)-mal mit einer Mantissenlänge von \(\left(i
\frac{n}{k}\right)_{i \in \{0, \ldots, k\}}\) zu einer festen Position
ausgewertet und ihr benötigter Speicherbedarf wird festgehalten.

\begin{figure}[H]
    \centering
    \includesvg[width=0.6\textwidth]{figures/memory-usage.svg}
    \caption{
        Speicherbedarf der verschiedenen Pi-Annäherungsfolgen in Abhängigkeit
        von der Anzahl der Dezimalstellen mit \(n = 512\) (Parameter
        \texttt{--digits} im Skript) und \(k = 20\) (Parameter
        \texttt{--samples} im Skript).
    }
    \label{fig:memory-usage}
\end{figure}

\begin{observation}
    Die Analyse zeigt, dass der Speicherbedarf mit zunehmender Anzahl der
    Dezimalstellen für alle Sequenzen linear ansteigt. Dabei ist der
    Speicherbedarf der Gauss-Legendre-Sequenz am höchsten, gefolgt von der
    Chudnovsky-Sequenz. Die Leibniz- und Monte-Carlo-Sequenzen zeigen einen
    vergleichsweise geringeren Anstieg des Speicherbedarfs, was mit deren
    relativ wenigen internen Variablen zusammenhängt. Besonders hervorzuheben
    ist, dass die Monte-Carlo-Sequenz den geringsten Speicherbedarf aufweist,
    was auf ihre probabilistische Natur und geringere Genauigkeit bei höherer
    Präzision hinweist.
\end{observation}

\section{Auswertung}

\section{Zusammenfassung}

\printbibliography

\end{document}
